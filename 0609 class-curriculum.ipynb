{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc66ad63-3fbf-43d6-b0ef-0ebd87f99ce3",
   "metadata": {},
   "source": [
    "Run HTML in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139a58e-463e-40f1-9396-570b55f25288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div>\n",
    "    <video width=400 controls=\"controls\">\n",
    "        <source src=\"assets/video/ex1.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    <video width=400 controls=\"controls\">\n",
    "        <source src=\"assets/video/ex2.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8045e-ee31-46e0-899e-9c756bfb712b",
   "metadata": {},
   "source": [
    "Create a VideoCapture object from an Ocean Exploration Challenge video clip and operate (level adjust and blur) on each frame of the video.\n",
    "The VideoCapture object will have 9000 frames (5m x 60sec/m x 30fps), or 2,073,600,000 RGB pixels (640 x 360 x 9000). That's a lot of operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aaf604-22ba-427e-b615-cf96a139e086",
   "metadata": {},
   "source": [
    "Operate on the original color video clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004feb32-b8cc-4088-ae4c-89719e3fb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# create a VideoCapture object\n",
    "cap = cv2.VideoCapture('assets/video/ex1.mp4')\n",
    "\n",
    "i = 0\n",
    "\n",
    "# loop until the end of the video\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    # capture the video frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # adjust the image levels\n",
    "    frame_adj = cv2.convertScaleAbs(frame, alpha=1.95, beta=0)   # 1-3, 0-100\n",
    "    \n",
    "    # blur the image\n",
    "    frame_blur = cv2.GaussianBlur(frame_adj, (9,9), 0)\n",
    "    \n",
    "    # DRAW A RECTANGLE\n",
    "    cv2.rectangle(frame_blur,(100+i,50+i),(300+i,150+i),color=(0,0,255),thickness=2)\n",
    "    i = i + 1\n",
    "    cv2.putText(frame_blur, text=\"Shrimp Video\", org=(20,40), fontFace=3, fontScale=1, color=(255,255,255), thickness=2)\n",
    "    \n",
    "    # display the frame\n",
    "    #cv2.imshow('my video', frame_adj)\n",
    "    cv2.imshow('my video', frame_blur)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebef98-a9d1-4d1e-acc4-ff18697779ea",
   "metadata": {},
   "source": [
    "Operate on a grayscale conversion of the original color video clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71deb41-23f1-4bad-94f4-b9524fdcf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# create a VideoCapture object\n",
    "cap = cv2.VideoCapture('assets/video/ex1.mp4')\n",
    "\n",
    "# loop until the end of the video\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    # capture the video frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # adjust the image levels\n",
    "    gray_adj = cv2.convertScaleAbs(gray, alpha=1.95, beta=0)\n",
    "    \n",
    "    # blur the image\n",
    "    gray_blur = cv2.GaussianBlur(gray_adj, (9,9), 0)\n",
    "    \n",
    "    # display the frame\n",
    "    #cv2.imshow('my video', gray_adj)\n",
    "    cv2.imshow('my video', gray_blur)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a7732-f389-4725-90b8-c9f8f4117ce4",
   "metadata": {},
   "source": [
    "It is critical to understand two important concepts when learning computer vision and image processing techniques.\n",
    "* thresholding = segmenting foreground from background (which pixels belong to the foreground and which belong to the backbround)\n",
    "* edge detecting = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80447a51-44e6-410b-b68c-a5caa9af9251",
   "metadata": {},
   "source": [
    "Color separation for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e975a9f-d59d-4926-9e52-43174e4bb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# create a VideoCapture object\n",
    "cap = cv2.VideoCapture('assets/video/ex1.mp4')\n",
    "\n",
    "# loop until the end of the video\n",
    "while (cap.isOpened()):\n",
    "\n",
    "    # capture the video frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # split the color channels\n",
    "    (B, G, R) = cv2.split(frame)\n",
    "    #cv2.imshow(\"Red\", R)\n",
    "    #cv2.imshow(\"Green\", G)\n",
    "    #cv2.imshow(\"Blue\", B)\n",
    "    \n",
    "    # visualize each channel in color\n",
    "    zeros = np.zeros(frame.shape[:2], dtype=\"uint8\")\n",
    "    frame_R = cv2.merge([zeros, zeros, R])\n",
    "    frame_G = cv2.merge([zeros, G, zeros])\n",
    "    frame_B = cv2.merge([B, zeros, zeros])\n",
    "    cv2.imshow(\"Red\", frame_R)\n",
    "    #cv2.imshow(\"Green\", frame_G)\n",
    "    #cv2.imshow(\"Blue\", frame_B)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613c260-3c31-4f65-9593-59044dc2f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame.shape[:2])\n",
    "print(frame.shape)\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376568a6-9b94-4398-9887-57ab5ef79488",
   "metadata": {},
   "source": [
    "Pre-process the image:\n",
    "* Get the red data from the frame.\n",
    "* Convert the frame to grayscale.\n",
    "* Apply a 7×7 Gaussian blur. Applying Gaussian blurring helps remove some of the high frequency edges in the image that we are not concerned with and allow us to obtain a more “clean” segmentation.\n",
    "* Apply the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760f880-b692-4688-b51a-672387b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# create a VideoCapture object\n",
    "cap = cv2.VideoCapture('assets/video/ex1.mp4')\n",
    "\n",
    "# loop until the end of the video\n",
    "while (cap.isOpened()):\n",
    "    cap.set(1, 500)\n",
    "\n",
    "    # capture the video frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # adjust the image levels\n",
    "    frame_adj = cv2.convertScaleAbs(frame, alpha=1.95, beta=0)   # 1-3, 0-100\n",
    "    \n",
    "    # split the color channels\n",
    "    (B, G, R) = cv2.split(frame)\n",
    "    #cv2.imshow(\"Red\", R)\n",
    "    #cv2.imshow(\"Green\", G)\n",
    "    #cv2.imshow(\"Blue\", B)\n",
    "    \n",
    "    # adjust the image levels\n",
    "    R_adj = cv2.convertScaleAbs(R, alpha=2.00, beta=0)\n",
    "    \n",
    "    # blur the image\n",
    "    R_blur = cv2.GaussianBlur(R_adj, (7,7), 0)\n",
    "    \n",
    "    # simple threshold\n",
    "    #(T, R_thresh) = cv2.threshold(R_blur, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "    (T, R_thresh) = cv2.threshold(R_blur, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # view only the masked regions in the image\n",
    "    R_masked = cv2.bitwise_and(R_adj, R_adj, mask=R_thresh)\n",
    "    frame_masked = cv2.bitwise_and(frame_adj, frame_adj, mask=R_thresh)\n",
    "    \n",
    "    # using cv2.Canny() for edge detection.\n",
    "    edge_detect = cv2.Canny(R_masked, 100, 200)\n",
    "    \n",
    "    # display the frame\n",
    "    #cv2.imshow('my video', R)\n",
    "    #cv2.imshow('my video', R_qadj)\n",
    "    #cv2.imshow('my video', R_blur)\n",
    "    cv2.imshow('my video', R_thresh)\n",
    "    #cv2.imshow('my video', R_masked)\n",
    "    #cv2.imshow('my video', frame_masked)\n",
    "    #cv2.imshow('my video', edge_detect)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda42d70-9e0d-4f14-9460-a0629e64ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e1b7c-dd16-4682-a4d6-8d79f06f50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "out = cv2.VideoWriter('assets/output.mp4', -1, 20.0, (640,480))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        frame = cv2.flip(frame,0)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2404d05e-e45e-48ec-93fc-9a3978c94a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 42, 200, 126)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#read image\n",
    "img_raw = cv2.imread('assets/0922-1.tiff')\n",
    "\n",
    "#select ROI function\n",
    "roi = cv2.selectROI(img_raw)\n",
    "\n",
    "#print rectangle points of selected roi\n",
    "print(roi)\n",
    "\n",
    "#Crop selected roi from raw image\n",
    "roi_cropped = img_raw[int(roi[1]):int(roi[1]+roi[3]), int(roi[0]):int(roi[0]+roi[2])]\n",
    "\n",
    "#show cropped image\n",
    "cv2.imshow(\"ROI\", roi_cropped)\n",
    "\n",
    "#cv2.imwrite(\"crop.jpeg\",roi_cropped)\n",
    "\n",
    "#hold window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1270b-2f8f-41f4-9faa-7a4c074d9beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
